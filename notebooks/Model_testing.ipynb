{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing \n",
    "\n",
    "### Metrics calculation on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext jupyter_black\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../libraries\")\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from libraries.utils import load_yaml\n",
    "from libraries.MLExperiment import MLExperiment\n",
    "\n",
    "from libraries.utils import extract_simulations_from_folders\n",
    "from libraries.MLExperiment import MLExperiment\n",
    "\n",
    "from QualityController import QualityController as QC\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Accessing variables\n",
    "data_folder = os.getenv(\"DATA_FOLDER\")\n",
    "saved_objects_folder = os.getenv(\"SAVED_OBJECTS_FOLDER\")\n",
    "project_name = os.getenv(\"PROJECT_NAME\")\n",
    "wandb_dir = os.getenv(\"WANDB_DIR\")\n",
    "yaml_folder = os.getenv(\"YAML_FOLDER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_name = \"Tuindorp development GNN.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = Path(yaml_folder) / yaml_name\n",
    "yaml_data = load_yaml(yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_simulations_path = Path(data_folder) / yaml_data[\"network\"] / \"simulations\" / \"testing\"\n",
    "\n",
    "inp_path = (\n",
    "    Path(data_folder)\n",
    "    / yaml_data[\"network\"]\n",
    "    / \"networks\"\n",
    "    / (yaml_data[\"network\"] + \".inp\")\n",
    ")\n",
    "\n",
    "test_sims = extract_simulations_from_folders(test_simulations_path, inp_path, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the experiment with the respective configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Testing performances\" + yaml_name, mode=\"disabled\", config=yaml_data\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "ml_experiment = MLExperiment(config, data_folder, saved_objects_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = QC(ml_experiment.model, ml_experiment.normalizer, test_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the metrics for each test simulation.\n",
    "  In this case, we demonstrate the calculation of the metrics with the Coefficient of determination (COD).\n",
    "\n",
    "  Other metrics can be calculated in a similar way. For example:\n",
    "  - Mean Squared Error (MSE)\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Root Mean Squared Error (RMSE)\n",
    "  - Mean Absolute Percentage Error (MAPE)\n",
    "  - Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "\n",
    "The second argument, 'overall', refers to the type of COD calculation. In this case, 'overall' calculates the COD for the entire network; instead of calculating the metric per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = qc.test_all_simulations_with_metric([(\"COD\", \"overall\")])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the execution time in seconds for each simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qc.execution_times) # Execution times of the simulations in seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_windows_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
